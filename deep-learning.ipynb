{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; font-weight: bold; font-size: 300%\">Projet INF442</div>                                                        <br /> \n",
    "<div style=\"text-align: center; font-weight: bold; font-size: 180%\"> Deep Learning </div>                                                        <br />  \n",
    "<div style=\"text-align: center; font-size: 150%\">École Polytechnique, mai 2020</div><br />  \n",
    "<div style=\"text-align: center; font-size: 120%\">Paul Calot et Jean-Charles Layoun</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apprentissage sur le jeu de données créé "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importation des données \n",
    "import torch\n",
    "import numpy as np\n",
    "import tqdm\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_1_1 = \"trainingdata_1_aller.csv\"\n",
    "path_1_2 = \"trainingdata_1_retour.csv\"\n",
    "path_2_1 = \"trainingdata_2_aller.csv\"\n",
    "path_2_2 = \"trainingdata_2_retour.csv\"\n",
    "path_3_1 = \"trainingdata_3_aller.csv\"\n",
    "path_3_2 = \"trainingdata_3_retour.csv\"\n",
    "path_4_1 = \"trainingdata_4_aller.csv\"\n",
    "path_4_2 = \"trainingdata_4_retour.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1.000000000000000000e+00</th>\n",
       "      <th>1.000000000000000000e+00.1</th>\n",
       "      <th>5.600000000000000000e+02</th>\n",
       "      <th>3.000000000000000000e+00</th>\n",
       "      <th>0.000000000000000000e+00</th>\n",
       "      <th>2.000000000000000000e+00</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>560.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>560.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>576.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>576.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>591.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1844</th>\n",
       "      <td>46.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>799.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1845</th>\n",
       "      <td>46.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>799.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1846</th>\n",
       "      <td>46.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>814.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1847</th>\n",
       "      <td>46.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>814.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1848</th>\n",
       "      <td>46.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>829.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1849 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      1.000000000000000000e+00  1.000000000000000000e+00.1  \\\n",
       "0                          1.0                         1.0   \n",
       "1                          1.0                         1.0   \n",
       "2                          1.0                         1.0   \n",
       "3                          1.0                         1.0   \n",
       "4                          1.0                         1.0   \n",
       "...                        ...                         ...   \n",
       "1844                      46.0                         1.0   \n",
       "1845                      46.0                         1.0   \n",
       "1846                      46.0                         1.0   \n",
       "1847                      46.0                         1.0   \n",
       "1848                      46.0                         1.0   \n",
       "\n",
       "      5.600000000000000000e+02  3.000000000000000000e+00  \\\n",
       "0                        560.0                       3.0   \n",
       "1                        560.0                       3.0   \n",
       "2                        576.0                      19.0   \n",
       "3                        576.0                      19.0   \n",
       "4                        591.0                      34.0   \n",
       "...                        ...                       ...   \n",
       "1844                     799.0                      89.0   \n",
       "1845                     799.0                      89.0   \n",
       "1846                     814.0                      89.0   \n",
       "1847                     814.0                      89.0   \n",
       "1848                     829.0                      88.0   \n",
       "\n",
       "      0.000000000000000000e+00  2.000000000000000000e+00  \n",
       "0                          0.0                       3.0  \n",
       "1                          0.0                       4.0  \n",
       "2                          0.0                       0.0  \n",
       "3                          0.0                       1.0  \n",
       "4                          0.0                       1.0  \n",
       "...                        ...                       ...  \n",
       "1844                       0.0                       0.0  \n",
       "1845                       0.0                       1.0  \n",
       "1846                       0.0                       0.0  \n",
       "1847                       0.0                       1.0  \n",
       "1848                       0.0                       0.0  \n",
       "\n",
       "[1849 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datacsv = pd.read_csv(path_1_1)\n",
    "datacsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = pd.read_csv(path).to_numpy()\n",
    "#np.random.shuffle(data)\n",
    "data1 = pd.read_csv(path_1_1).to_numpy()\n",
    "data2 = pd.read_csv(path_2_1).to_numpy()\n",
    "data3 = pd.read_csv(path_3_1).to_numpy()\n",
    "data4 = pd.read_csv(path_4_1).to_numpy()\n",
    "data_aller = np.concatenate((data1,data2,data3,data4))\n",
    "\n",
    "data1r = pd.read_csv(path_1_2).to_numpy()\n",
    "data2r = pd.read_csv(path_2_2).to_numpy()\n",
    "data3r = pd.read_csv(path_3_2).to_numpy()\n",
    "data4r = pd.read_csv(path_4_2).to_numpy()\n",
    "data_retour = np.concatenate((data1r,data2r,data3r,data4r))\n",
    "\n",
    "# may need reprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille du dataset aller : 7398\n",
      "Taille du dataset aller : 6936\n"
     ]
    }
   ],
   "source": [
    "print(\"Taille du dataset aller : \" + str(len(data_aller)))\n",
    "print(\"Taille du dataset aller : \" + str(len(data_retour)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting :\n",
    "data_taken = 3\n",
    "def preprocessed(data):\n",
    "    \"\"\"\n",
    "    this algorithm aims to prepare data for the deeplearning algo that follows\n",
    "    that implie :\n",
    "        0. Shuffling the data\n",
    "        1. Splitting the data into a training set and a testing set\n",
    "        2. Splitting ground truth / features\n",
    "        3. Converting to torch.Tensors\n",
    "        4. Normalizing data\n",
    "        4. Converting into datasets\n",
    "        5. Returning the datasets\n",
    "    \"\"\"\n",
    "        # shuffling\n",
    "    np.random.shuffle(data)\n",
    "        \n",
    "        # splitting 60 - 40\n",
    "    splittinglimit = int(0.6*len(data))\n",
    "    training_set = data[:splittinglimit]\n",
    "    testing_set = data[splittinglimit:]\n",
    "        \n",
    "        # splitting ground truth / features\n",
    "\n",
    "    x_train_ = np.zeros((len(training_set),data_taken))\n",
    "    y_train_ = np.zeros(len(training_set))\n",
    "    for k in range(len(training_set)):\n",
    "        x_train_[k][0] = training_set[k][0] # station\n",
    "        x_train_[k][1] = training_set[k][2] # estimation initiale \n",
    "\n",
    "        x_train_[k][2] = training_set[k][4] # si en retard ou pas\n",
    "        y_train_[k] = training_set[k][5]\n",
    "    \n",
    "    x_test_ = np.zeros((len(testing_set),data_taken))\n",
    "    y_test_ = np.zeros(len(testing_set))\n",
    "    for k in range(len(testing_set)): \n",
    "        x_test_[k][0] = testing_set[k][0]\n",
    "        x_test_[k][1] = testing_set[k][2]\n",
    "        x_test_[k][2] = testing_set[k][4]\n",
    "        y_test_[k] = testing_set[k][5]\n",
    "    \n",
    "        # converting to torch tensors\n",
    "    x_train = torch.Tensor(x_train_)\n",
    "    y_train = torch.Tensor(y_train_)\n",
    "    x_test = torch.Tensor(x_test_)\n",
    "    y_test = torch.Tensor(y_test_)\n",
    "    \n",
    "        # normalizing\n",
    "    x_test = nn.functional.normalize(x_test, p=2, dim=0)\n",
    "    x_train = nn.functional.normalize(x_train, p=2, dim=0)\n",
    "    \n",
    "        # creating datasetabs    \n",
    "    training_dataset = torch.utils.data.TensorDataset(x_train,y_train)\n",
    "    testing_dataset = torch.utils.data.TensorDataset(x_test,y_test)\n",
    "    \n",
    "    return (training_dataset, testing_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset_aller, testing_dataset_aller = preprocessed(data_aller)\n",
    "training_dataset_retour,testing_dataset_retour = preprocessed(data_retour)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quantité de données\n",
      "4438\n",
      "2960\n",
      "4161\n",
      "2775\n",
      "(tensor([0.0073, 0.0213, 0.0000]), tensor(4.))\n"
     ]
    }
   ],
   "source": [
    "print(\"quantité de données\")\n",
    "print(len(training_dataset_aller))\n",
    "print(len(testing_dataset_aller))\n",
    "print(len(training_dataset_retour))\n",
    "print(len(testing_dataset_retour))\n",
    "print(training_dataset_aller[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Création d'un réseau de neurone\n",
    "\n",
    "\n",
    "|                         | nombre de données entrainement | nombre de données test | \n",
    "| ------------------------|--------------------------------| -----------------------|\n",
    "| aller           |        4438                |       2960          | \n",
    "| retour          |                 4161         |    2775                | \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definition of a testloader\n",
    "#testloader = torch.utils.data.DataLoader(test_data, len(test_data), shuffle=True)\n",
    "\n",
    "def train_epoch(net, opt, criterion, trainloader, batch_size=50): # pour entraîner sur un epoch\n",
    "    net.train()\n",
    "    losses = []\n",
    "    for x_batch, y_batch in trainloader:\n",
    "        opt.zero_grad()\n",
    "        # Forward\n",
    "        y_comp = net(x_batch)\n",
    "        # Compute diff\n",
    "        loss = criterion(y_comp, y_batch)\n",
    "        # Compute gradients\n",
    "        loss.backward()\n",
    "        # update weights\n",
    "        opt.step()\n",
    "        losses.append(loss.data.numpy())\n",
    "    return losses\n",
    "\n",
    "def accuracy(net, dataset, mins): # pour calculer la précision\n",
    "    net.eval() # pass to model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for data in dataset:\n",
    "        points, labels = data\n",
    "        outputs = net(points)\n",
    "        #print(outputs, labels)\n",
    "        total += labels.size(0)\n",
    "        \n",
    "        for k in range(labels.size(0)):\n",
    "            if(abs(int(outputs[k].item()) - labels[k].item()) < mins+1):\n",
    "                correct +=1\n",
    "    net.train() # pass the model to training mode\n",
    "    return correct / total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, deepness = 10, weights_nb = 256):\n",
    "        super(Net, self).__init__()\n",
    "        self.inputlayer = nn.Linear(data_taken,weights_nb)\n",
    "        liste = []\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        for k in range(deepness):\n",
    "            liste.append(nn.Linear(weights_nb,weights_nb))\n",
    "            liste.append(self.relu)\n",
    "        self.hiddenlayers = nn.Sequential(*liste)\n",
    "        self.outputlayer = nn.Linear(weights_nb,1)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # intput layer\n",
    "        x = self.inputlayer(x)\n",
    "        x = self.relu(x)\n",
    "        # hidden layers\n",
    "        x = self.hiddenlayers(x)\n",
    "        # output layer\n",
    "        x = self.outputlayer(x)\n",
    "       #x = self.sigmoid(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (inputlayer): Linear(in_features=3, out_features=256, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (sigmoid): Sigmoid()\n",
       "  (hiddenlayers): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (7): ReLU()\n",
       "    (8): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (9): ReLU()\n",
       "    (10): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (11): ReLU()\n",
       "    (12): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (13): ReLU()\n",
       "    (14): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (15): ReLU()\n",
       "    (16): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (17): ReLU()\n",
       "    (18): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (19): ReLU()\n",
       "  )\n",
       "  (outputlayer): Linear(in_features=256, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Net() # test\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(train_data, test_data, model, batch_size = 64, epochs = 10, lr = 1e-4):\n",
    "    \"\"\"\n",
    "    this function returns a list of precision - 1 precision / epoch - for the given training and testing data, for the given model\n",
    "    \"\"\"\n",
    "    testloader = torch.utils.data.DataLoader(test_data, 1, shuffle=False)\n",
    "    trainloader = torch.utils.data.DataLoader(train_data, batch_size, shuffle=True)\n",
    "    net = model\n",
    "    \n",
    "    # criterion for the last and optimizer\n",
    "    def criterion(x,y):\n",
    "        n = len(x)\n",
    "        s = 0\n",
    "        for k in range(n):\n",
    "            s+=(x[k]-y[k])*(x[k]-y[k])\n",
    "        return s/n\n",
    "    \n",
    "    opt = torch.optim.Adam(net.parameters(),lr,betas=(0.9, 0.999),eps=1e-08)\n",
    "    #\n",
    "    accuracy_list = []\n",
    "    #accuracy_list_train = []\n",
    "    for k in tqdm.tqdm(range(epochs)):\n",
    "        net.train()\n",
    "        train_epoch(net, opt, criterion, trainloader, batch_size = batch_size)\n",
    "        prec=accuracy(net, dataset = testloader, mins = 0)\n",
    "        accuracy_list.append(prec)\n",
    "        #accuracy_list_train.append(accuracy(net, dataset = trainloader))\n",
    "    return (net , accuracy_list) # this way we can keep the trained net and test it some more after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:10<00:00,  1.09s/it]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "training_dataset_aller\n",
    "testing_dataset_aller\n",
    "training_dataset_retour\n",
    "testing_dataset_retour\n",
    "\"\"\"\n",
    " # the NN that we'll use\n",
    "net = Net(deepness = 1, weights_nb = 128)\n",
    "net, accuracy_list = test_model(training_dataset_aller, testing_dataset_aller, net, batch_size = 16, epochs = 10, lr=0.0001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (inputlayer): Linear(in_features=3, out_features=128, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (sigmoid): Sigmoid()\n",
       "  (hiddenlayers): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (outputlayer): Linear(in_features=128, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2760135135135135, 0.2760135135135135, 0.27533783783783783, 0.23141891891891891, 0.2722972972972973, 0.2760135135135135, 0.23141891891891891, 0.23141891891891891, 0.23141891891891891, 0.23141891891891891]\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test sur le le testing dataset aller : \n",
      "précision RATP à 0 min : 0.16925675675675675\n",
      "précision RATP à 1 min : 0.4452702702702703\n",
      "précision RATP à 3 min : 0.8057432432432432\n",
      "précision RATP à 10 min : 0.985472972972973\n"
     ]
    }
   ],
   "source": [
    "# test précision RATP :\n",
    "nb_0 = 0\n",
    "nb_1 = 0\n",
    "nb_3 = 0\n",
    "nb_10 = 0\n",
    "total = 0\n",
    "for k in range(len(testing_dataset_aller)):\n",
    "    total +=1\n",
    "    if(testing_dataset_aller[k][1]<1): # exact\n",
    "        nb_0+=1\n",
    "    if(testing_dataset_aller[k][1]<2): # à 3 mins\n",
    "        nb_1+=1\n",
    "    if(testing_dataset_aller[k][1]<4): # à 3 mins\n",
    "        nb_3+=1\n",
    "    if(testing_dataset_aller[k][1]<11): # à 3 mins\n",
    "        nb_10+=1\n",
    "print(\"Test sur le le testing dataset aller : \")\n",
    "print(\"précision RATP à 0 min : \" + str(nb_0/total))\n",
    "print(\"précision RATP à 1 min : \" + str(nb_1/total))\n",
    "print(\"précision RATP à 3 min : \" + str(nb_3/total))\n",
    "print(\"précision RATP à 10 min : \" + str(nb_10/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test sur le le testing dataset aller : \n",
      "précision modèle à 0 min : 0.23141891891891891\n",
      "précision modèle à 1 min : 0.6364864864864865\n",
      "précision modèle à 3 min : 0.9074324324324324\n",
      "précision modèle à 10 min : 0.9824324324324324\n"
     ]
    }
   ],
   "source": [
    "# précision notre modèle à \n",
    "print(\"Test sur le le testing dataset aller : \")\n",
    "testloader = torch.utils.data.DataLoader(testing_dataset_aller, 1, shuffle=False)\n",
    "print(\"précision modèle à 0 min : \" + str(accuracy(net, testloader, 0)))\n",
    "print(\"précision modèle à 1 min : \" + str(accuracy(net, testloader, 1)))\n",
    "print(\"précision modèle à 3 min : \" + str(accuracy(net, testloader, 3)))\n",
    "print(\"précision modèle à 10 min : \" + str(accuracy(net, testloader, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test sur le training dataset aller : \n",
      "précision modèle à 0 min : 0.2273546642631816\n",
      "précision modèle à 1 min : 0.6297881928796756\n",
      "précision modèle à 3 min : 0.9105452906714736\n",
      "précision modèle à 10 min : 0.9831004957187922\n"
     ]
    }
   ],
   "source": [
    "print(\"Test sur le training dataset aller : \")\n",
    "testloader = torch.utils.data.DataLoader(training_dataset_aller, 1, shuffle=False)\n",
    "print(\"précision modèle à 0 min : \" + str(accuracy(net, testloader, 0)))\n",
    "print(\"précision modèle à 1 min : \" + str(accuracy(net, testloader, 1)))\n",
    "print(\"précision modèle à 3 min : \" + str(accuracy(net, testloader, 3)))\n",
    "print(\"précision modèle à 10 min : \" + str(accuracy(net, testloader, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On est donc meilleur pour prédire sans \"gros retard\" que la RATP. Avec davantage de données, on pourrait probablement encore mieux faire !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Généralisation au sens retour ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test sur le testing dataset retour : \n",
      "précision RATP à 0 min : 0.19207207207207208\n",
      "précision RATP à 1 min : 0.3981981981981982\n",
      "précision RATP à 3 min : 0.7455855855855856\n",
      "précision RATP à 10 min : 0.9927927927927928\n"
     ]
    }
   ],
   "source": [
    "print(\"Test sur le testing dataset retour : \")\n",
    "nb_0 = 0\n",
    "nb_1 = 0\n",
    "nb_3 = 0\n",
    "nb_10 = 0\n",
    "total = 0\n",
    "for k in range(len(testing_dataset_retour)):\n",
    "    total +=1\n",
    "    if(testing_dataset_retour[k][1]<1): # exact\n",
    "        nb_0+=1\n",
    "    if(testing_dataset_retour[k][1]<2): # à 3 mins\n",
    "        nb_1+=1\n",
    "    if(testing_dataset_retour[k][1]<4): # à 3 mins\n",
    "        nb_3+=1\n",
    "    if(testing_dataset_retour[k][1]<11): # à 3 mins\n",
    "        nb_10+=1\n",
    "print(\"précision RATP à 0 min : \" + str(nb_0/total))\n",
    "print(\"précision RATP à 1 min : \" + str(nb_1/total))\n",
    "print(\"précision RATP à 3 min : \" + str(nb_3/total))\n",
    "print(\"précision RATP à 10 min : \" + str(nb_10/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test sur le testing dataset retour : \n",
      "précision modèle à 0 min : 0.21153153153153154\n",
      "précision modèle à 1 min : 0.5535135135135135\n",
      "précision modèle à 3 min : 0.8544144144144145\n",
      "précision modèle à 10 min : 0.9978378378378379\n"
     ]
    }
   ],
   "source": [
    "print(\"Test sur le testing dataset retour : \")\n",
    "testloader = torch.utils.data.DataLoader(testing_dataset_retour, 1, shuffle=False)\n",
    "print(\"précision modèle à 0 min : \" + str(accuracy(net, testloader, 0)))\n",
    "print(\"précision modèle à 1 min : \" + str(accuracy(net, testloader, 1)))\n",
    "print(\"précision modèle à 3 min : \" + str(accuracy(net, testloader, 3)))\n",
    "print(\"précision modèle à 10 min : \" + str(accuracy(net, testloader, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les résultats précédents sont à prendre avec des pincettes. Les données extraites après le processing sont partiellement biaisés (et il est difficile d'apprécier ce biais) et bruités également.\n",
    "\n",
    "Ici le choix qui a été fait est de considérer comme période de temps pertinente la journée. C'est-à-dire que l'on s'attend à avoir une répétition de ces données d'une journée à l'autre. Ce choix est en quelque sort contraints au vu des données dont on dispose.\n",
    "\n",
    "Dans l'idéal, on ferait des prédictions sur des périodes de temps de l'ordre de la semaine. En effet, l'échelle de l'année est plus pertinente encore, mais la quantité de données que cela représente pour pouvoir entraîner un réseau de neurones n'est pas vraisemblable. L'échelle de la semaine est \"l'entre deux\".\n",
    "\n",
    "Note : les précisions peuvent changer d'une exécution à une autre. Cela est du à l'initialisation aléatoire des poids dans le réseau de neurones."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
